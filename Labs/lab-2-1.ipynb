{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Lab 2.1: Applying ML to an NLP Problem\n", "\n", "In this lab, you will use the built-in machine learning (ML) model in Amazon SageMaker, __LinearLearner__, to predict the __isPositive__ field of the review dataset.\n", "\n", "## Introducing the business scenario\n", "You work for an online retail store that wants to improve customer engagement for customers who have posted negative reviews. The company wants to detect negative reviews and assign these reviews to a customer service agent to address.\n", "\n", "You are tasked with solving part of this problem by using ML to detect negative reviews. You were given access to a dataset that contains reviews, which have been classified as positive or negative. You will use this dataset to train an ML model to predict the sentiment of new reviews.\n", "\n", "## About this dataset\n", "The [AMAZON-REVIEW-DATA-CLASSIFICATION.csv](https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp/tree/master/data/examples) file contains actual reviews of products, and these reviews include both text data and numeric data. Each review is labeled as _positive (1)_ or _negative (0)_.\n", "\n", "The dataset contains the following features:\n", "* __reviewText:__ Text of the review\n", "* __summary:__ Summary of the review\n", "* __verified:__ Whether the purchase was verified (True or False)\n", "* __time:__ UNIX timestamp for the review\n", "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n", "* __isPositive:__ Whether the review is positive or negative (1 or 0)\n", "\n", "The dataset for this lab is being provided to you by permission of Amazon and is subject to the terms of the Amazon License and Access (available at https://www.amazon.com/gp/help/customer/display.html?nodeId=201909000). You are expressly prohibited from copying, modifying, selling, exporting, or using this dataset in any way other than for the purpose of completing this course.\n", "\n", "## Lab steps\n", "\n", "To complete this lab, you will follow these steps:\n", "\n", "1. [Reading the dataset](#1.-Reading-the-dataset)\n", "2. [Performing exploratory data analysis](#2.-Performing-exploratory-data-analysis)\n", "3. [Text processing: Removing stopwords and stemming](#3.-Text-processing:-Removing-stopwords-and-stemming)\n", "4. [Splitting training, validation, and test data](#4.-Splitting-training,-validation,-and-test-data)\n", "5. [Processing data with pipelines and a ColumnTransformer](#5.-Processing-data-with-pipelines-and-a-ColumnTransformer)\n", "6. [Training a classifier with a built-in SageMaker algorithm](#6.-Training-a-classifier-with-a-built-in-SageMaker-algorithm)\n", "7. [Evaluating the model](#7.-Evaluating-the-model)\n", "8. [Deploying the model to an endpoint](#8.-Deploying-the-model-to-an-endpoint)\n", "9. [Testing the endpoint](#9.-Testing-the-endpoint)\n", "10. [Cleaning up model artifacts](#10.-Cleaning-up-model-artifacts)\n", "    \n", "## Submitting your work\n", "\n", "1. In the lab console, choose **Submit** to record your progress and when prompted, choose **Yes**.\n", "\n", "1. If the results don't display after a couple of minutes, return to the top of these instructions and choose **Grades**.\n", "\n", "     **Tip**: You can submit your work multiple times. After you change your work, choose **Submit** again. Your last submission is what will be recorded for this lab.\n", "\n", "1. To find detailed feedback on your work, choose **Details** followed by **View Submission Report**.  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Start by installing/upgrading pip, sagemaker, and scikit-learn.\n", "\n", "[scikit-learn](https://scikit-learn.org/stable/) is an open source machine learning library. It provides various tools for model fitting, data preprocessing, model selection and evaluation and many other utilities."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Upgrade dependencies\n", "!pip install --upgrade pip\n", "!pip install --upgrade scikit-learn\n", "!pip install --upgrade sagemaker"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Reading the dataset\n", "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n", "\n", "You will use the __pandas__ library to read the dataset. [Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) is a popular python library that is used for data analysis. It provides data manipulation, cleaning, and data wrangling features as well as visualizations."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "\n", "df = pd.read_csv('../data/AMAZON-REVIEW-DATA-CLASSIFICATION.csv')\n", "\n", "print('The shape of the dataset is:', df.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Look at the first five rows in the dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.head(5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can change the options in the notebook to display more of the text data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pd.options.display.max_rows\n", "pd.set_option('display.max_colwidth', None)\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can look at specific entries if needed."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(df.loc[[580]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It's good to know what data types you are dealing with. You can use `dtypes` on the dataframe to display the types."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.dtypes"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Performing exploratory data analysis\n", "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n", "\n", "You will now look at the target distribution for your dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df['isPositive'].value_counts()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The business problem is concerned with finding the negative reviews (_0_). However, the model tuning for linear learner defaults to finding positive values (_1_). You can make this process run more smoothly by switching the negative values (_0_) and positive values (_1_). By doing so, you can tune the model more easily."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df.replace({0:1, 1:0})\n", "df['isPositive'].value_counts()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check the number of missing values:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.isna().sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The text fields have missing values. Typically, you would decide what to do with these missing values. You could remove the data or fill it with some standard text. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Text processing: Removing stopwords and stemming\n", "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n", "\n", "In this task, you will remove some of the stopwords, and perform stemming on the text data. You are normalizing the data to reduce the amount of different information you have to deal with.\n", "\n", "[nltk](https://www.nltk.org/) is a popular platform for working with human language data. It provides interfaces and functions for processing text for classification, tokenization, stemming, tagging, parsin, and semantic reasoning. \n", "\n", "Once imported, you can download only the functionality you need. In this example, you will use:\n", "\n", "- **punkt** is a sentence tokenizer\n", "- **stopwords** provides a list of stopwords you can use."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install the library and functions\n", "import nltk\n", "nltk.download('punkt')\n", "nltk.download('stopwords')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You will create the processes for removing stopwords and cleaning the text in the following section. The Natural Language Toolkit (NLTK) library provides a list of common stopwords. You will use the list, but you will first remove some of the words from that list. The stopwords that you keep in the text are useful for determining sentiment."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import nltk, re\n", "from nltk.corpus import stopwords\n", "from nltk.stem import SnowballStemmer\n", "from nltk.tokenize import word_tokenize\n", "\n", "# Get a list of stopwords from the NLTK library\n", "stop = stopwords.words('english')\n", "\n", "# These words are important for your problem. You don't want to remove them.\n", "excluding = ['against', 'not', 'don', 'don\\'t','ain', 'are', 'aren\\'t', 'could', 'couldn\\'t',\n", "             'did', 'didn\\'t', 'does', 'doesn\\'t', 'had', 'hadn\\'t', 'has', 'hasn\\'t', \n", "             'have', 'haven\\'t', 'is', 'isn\\'t', 'might', 'mightn\\'t', 'must', 'mustn\\'t',\n", "             'need', 'needn\\'t','should', 'shouldn\\'t', 'was', 'wasn\\'t', 'were', \n", "             'weren\\'t', 'won\\'t', 'would', 'wouldn\\'t']\n", "\n", "# New stopword list\n", "stopwords = [word for word in stop if word not in excluding]\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The snowball stemmer will stem words. For example, 'walking' will be stemmed to 'walk'."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["snow = SnowballStemmer('english')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You must perform a few other normalization tasks on the data. The following function will:\n", "\n", "- Replace any missing values with an empty string\n", "- Convert the text to lowercase\n", "- Remove any leading or training whitespace\n", "- Remove any extra space and tabs\n", "- Remove any HTML markup\n", "\n", "In the `for` loop, any words that are __NOT__ numeric, longer than 2 characters, and not part of the list of stop words will be kept and returned."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def process_text(texts): \n", "    final_text_list=[]\n", "    for sent in texts:\n", "        \n", "        # Check if the sentence is a missing value\n", "        if isinstance(sent, str) == False:\n", "            sent = ''\n", "            \n", "        filtered_sentence=[]\n", "        \n", "        sent = sent.lower() # Lowercase \n", "        sent = sent.strip() # Remove leading/trailing whitespace\n", "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n", "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n", "        \n", "        for w in word_tokenize(sent):\n", "            # Applying some custom filtering here, feel free to try different things\n", "            # Check if it is not numeric and its length>2 and not in stopwords\n", "            if(not w.isnumeric()) and (len(w)>2) and (w not in stopwords):  \n", "                # Stem and add to filtered list\n", "                filtered_sentence.append(snow.stem(w))\n", "        final_string = \" \".join(filtered_sentence) # Final string of cleaned words\n", " \n", "        final_text_list.append(final_string)\n", "        \n", "    return final_text_list"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Splitting training, validation, and test data\n", "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n", "\n", "In this step, you will split the dataset into training (80 percent), validation (10 percent), and test (10 percent) by using the sklearn [__train_test_split()__](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n", "\n", "The training data will be used to train the model which is then tested with the test data. The validation set is used once the model has been trained to give you metrics on how the model might perform on real data. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_val, y_train, y_val = train_test_split(df[['reviewText', 'summary', 'time', 'log_votes']],\n", "                                                  df['isPositive'],\n", "                                                  test_size=0.20,\n", "                                                  shuffle=True,\n", "                                                  random_state=324\n", "                                                 )\n", "\n", "X_val, X_test, y_val, y_test = train_test_split(X_val,\n", "                                                y_val,\n", "                                                test_size=0.5,\n", "                                                shuffle=True,\n", "                                                random_state=324)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With the dataset split, you can now run the `process_text` function defined above on each of the text features in the training, test, and validation sets."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Processing the reviewText fields')\n", "X_train['reviewText'] = process_text(X_train['reviewText'].tolist())\n", "X_val['reviewText'] = process_text(X_val['reviewText'].tolist())\n", "X_test['reviewText'] = process_text(X_test['reviewText'].tolist())\n", "\n", "print('Processing the summary fields')\n", "X_train['summary'] = process_text(X_train['summary'].tolist())\n", "X_val['summary'] = process_text(X_val['summary'].tolist())\n", "X_test['summary'] = process_text(X_test['summary'].tolist())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Processing data with pipelines and a ColumnTransformer\n", "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n", "\n", "You will often perform many tasks on data before you use it to train a model. These steps must also be done on any data that's used for inference after the model is deployed. A good way of organizing these steps is to define a _pipeline_. A pipeline is a collection of processing tasks that will be performed on the data. Different pipelines can be created to process different fields. Because you are working with both text and numeric data, you can define the following pipelines:\n", "\n", "   * For the numerical features pipeline, the __numerical_processor__ uses a MinMaxScaler. (You don't need to scale features when you use decision trees, but it's a good idea to see how to use more data transforms.) If you want to perform different types of processing on different numerical features, you should build different pipelines, like the ones that are shown for the two text features.\n", "   * For the text features pipeline, the __text_processor__ uses `CountVectorizer()` for the text fields.\n", "   \n", "The selective preparations of the dataset features are then put together into a collective ColumnTransformer, which will be used with in a pipeline along with an estimator. This process ensures that the transforms are performed automatically on the raw data when you fit the model or make predictions. (For example, when you evaluate the model on a validation dataset via cross-validation, or when you make predictions on a test dataset in the future.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Grab model features/inputs and target/output\n", "numerical_features = ['time',\n", "                      'log_votes']\n", "\n", "text_features = ['summary',\n", "                 'reviewText']\n", "\n", "model_features = numerical_features + text_features\n", "model_target = 'isPositive'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.impute import SimpleImputer\n", "from sklearn.preprocessing import MinMaxScaler\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.compose import ColumnTransformer\n", "\n", "### COLUMN_TRANSFORMER ###\n", "##########################\n", "\n", "# Preprocess the numerical features\n", "numerical_processor = Pipeline([\n", "    ('num_imputer', SimpleImputer(strategy='mean')),\n", "    ('num_scaler', MinMaxScaler()) \n", "                                ])\n", "# Preprocess 1st text feature\n", "text_processor_0 = Pipeline([\n", "    ('text_vect_0', CountVectorizer(binary=True, max_features=50))\n", "                                ])\n", "\n", "# Preprocess 2nd text feature (larger vocabulary)\n", "text_precessor_1 = Pipeline([\n", "    ('text_vect_1', CountVectorizer(binary=True, max_features=150))\n", "                                ])\n", "\n", "# Combine all data preprocessors from above (add more, if you choose to define more!)\n", "# For each processor/step specify: a name, the actual process, and finally the features to be processed\n", "data_preprocessor = ColumnTransformer([\n", "    ('numerical_pre', numerical_processor, numerical_features),\n", "    ('text_pre_0', text_processor_0, text_features[0]),\n", "    ('text_pre_1', text_precessor_1, text_features[1])\n", "                                    ]) \n", "\n", "### DATA PREPROCESSING ###\n", "##########################\n", "\n", "print('Datasets shapes before processing: ', X_train.shape, X_val.shape, X_test.shape)\n", "\n", "X_train = data_preprocessor.fit_transform(X_train).toarray()\n", "X_val = data_preprocessor.transform(X_val).toarray()\n", "X_test = data_preprocessor.transform(X_test).toarray()\n", "\n", "print('Datasets shapes after processing: ', X_train.shape, X_val.shape, X_test.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note how the number of features in the datasets went from 4 to 202."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(X_train[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Training a classifier with a built-in SageMaker algorithm\n", "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n", "\n", "In this step, you will call the Sagemaker `LinearLearner()` algorithm with the following options:\n", "* __Permissions -__ `role` is set to the AWS Identity and Access Management (IAM) role from the current environment.\n", "* __Compute power -__ You will use the `train_instance_count` parameter and the `train_instance_type` parameter. This example uses an `ml.m4.xlarge` resource for training. You can change the instance type depending on your needs. (For example, you could use GPUs for neural networks.) \n", "* __Model type -__ `predictor_type` is set to __`binary_classifier`__ because you are working with a binary classification problem. You could use __`multiclass_classifier`__ if three or more classes are involved, or you could use __`regressor`__ for a regression problem.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sagemaker\n", "\n", "# Call the LinearLearner estimator object\n", "linear_classifier = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n", "                                           instance_count=1,\n", "                                           instance_type='ml.m4.xlarge',\n", "                                           predictor_type='binary_classifier')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To set the training, validation, and test parts of the estimator, you can use the `record_set()` function of the `binary_estimator`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_records = linear_classifier.record_set(X_train.astype('float32'),\n", "                                            y_train.values.astype('float32'),\n", "                                            channel='train')\n", "val_records = linear_classifier.record_set(X_val.astype('float32'),\n", "                                          y_val.values.astype('float32'),\n", "                                          channel='validation')\n", "test_records = linear_classifier.record_set(X_test.astype('float32'),\n", "                                           y_test.values.astype('float32'),\n", "                                           channel='test')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The `fit()` function applies a distributed version of the Stochastic Gradient Descent (SGD) algorithm, and you are sending the data to it. The logs were disabled with `logs=False`. You can remove that parameter to see more details about the process. __This process takes about 3-4 minutes on an ml.m4.xlarge instance.__"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["linear_classifier.fit([train_records,\n", "                       val_records,\n", "                       test_records],\n", "                       logs=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Evaluating the model\n", "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n", "\n", "You can use SageMaker analytics to get some performance metrics (of your choosing) on the test set. This process doesn't require you to deploy the model. \n", "\n", "Linear learner provides metrics that are computed during training. You can use these metrics when tuning the model. The available metrics for the validation set are:\n", "\n", "- objective_loss - For a binary classification problem, this will be the mean value of the logistic loss for each epoch\n", "- binary_classification_accuracy - The accuracy of the final model on the dataset i.e. how many predictions did the model get right\n", "- precision - Quantifies the number of positive class predictions that are actually positive\n", "- recall - Quantifies the number of positive class predictions\n", "- binary_f_beta - The harmonic mean of the precision and recall metrics\n", "\n", "For this example, you are interested in how many predictions were correct. Using the **binary_classification_accuracy** metric seems appropriate."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sagemaker.analytics.TrainingJobAnalytics(linear_classifier._current_job_name, \n", "                                         metric_names = ['test:binary_classification_accuracy']\n", "                                        ).dataframe()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You should see a value of around 0.85. Your value will may be different, but should be around that value. This translates to the model accuractely predicting the correct answer 85% of the time. Depending upon the business case, you may need to tune the model further using a hyperparameter tuning job, or do some more feature engineering."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Deploying the model to an endpoint\n", "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n", "\n", "In this last part of this exercise, you will deploy your model to another instance of your choice. You can use this model in a production environment. Deployed endpoints can be used with other AWS services, such as AWS Lambda and Amazon API Gateway. If you are interested in learning more, see the following walkthrough: [Call an Amazon SageMaker model endpoint using Amazon API Gateway and AWS Lambda](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/).\n", "\n", "To deploy the model, run the following cell. You can use different instance types, such as: _ml.t2.medium_, _ml.c4.xlarge_), and others. __This process will take some time to complete (approximately 7-8 minutes).__"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "linear_classifier_predictor = linear_classifier.deploy(initial_instance_count = 1,\n", "                                                       instance_type = 'ml.c5.large'\n", "                                                      )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. Testing the endpoint\n", "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n", "\n", "Now that the endpoint is deployed, you will send the test data to it and get predictions from the data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "# Get test data in batch size of 25 and make predictions.\n", "prediction_batches = [linear_classifier_predictor.predict(batch)\n", "                      for batch in np.array_split(X_test.astype('float32'), 25)\n", "                     ]\n", "\n", "# Get a list of predictions\n", "print([pred.label['score'].float32_tensor.values[0] for pred in prediction_batches[0]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10. Cleaning up model artifacts\n", "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n", "\n", "You can run the following to delete the endpoint after you are done using it. \n", "\n", "**Tip:** - Remember that when using your own account, you will accrue charges if you don't delete the endpoint and other resources."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["linear_classifier_predictor.delete_endpoint()"]}, {"source": ["# Congratulations!\n", "\n", "In this lab, you looked at a very simple NLP problem. Using a labelled dataset, you used a simple tokenizer and encoder to generate the data required to train a linear learner model. You then deployed the model and performed some predictions. If you were doing this for real, you would likely need to obtain the data and label it for training. An alternative might be to use a pretrained algorithm or managed service. You would also likely tune the model further using a hyperparameter tuning job.\n", "\n", "You have completed this lab, and you can now end the lab by following the lab guide instructions."], "cell_type": "markdown", "metadata": {}}, {"source": ["\u00a92021 Amazon Web Services, Inc. or its affiliates. All rights reserved. This work may not be reproduced or redistributed, in whole or in part, without prior written permission from Amazon Web Services, Inc. Commercial copying, lending, or selling is prohibited. All trademarks are the property of their owners."], "cell_type": "markdown", "metadata": {}}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"instance_type": "ml.t3.medium", "kernelspec": {"display_name": "conda_python3", "language": "python", "name": "conda_python3"}, "language_info": {"codemirror_mode": {"name": "python", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "pyspark", "pygments_lexer": "python3", "version": "3.8.5"}, "metadata": {"interpreter": {"hash": "12bdb53ebf8de4a8c3e84b62f6391946884c7c7585d9344b706f290a85145ccc"}}}, "nbformat": 4, "nbformat_minor": 4}