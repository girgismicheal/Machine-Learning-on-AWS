{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Capstone Project: Bringing It All Together\n",
    "\n",
    "In this lab, you will bring together many of the tools and techniques that you have learned throughout this course into a final project. You can choose from many different paths to get to the solution. You could use AWS Managed Services, such as Amazon Comprehend, or use the Amazon SageMaker models. Have fun on whichever path you choose.\n",
    "\n",
    "### Business scenario\n",
    "\n",
    "You work for a training organization that recently developed an introductory course about machine learning (ML). The course includes more than 40 videos that cover a broad range of ML topics. You have been asked to create an application that will students can use to quickly locate and view video content by searching for topics and key phrases.\n",
    "\n",
    "You have downloaded all of the videos to an Amazon Simple Storage Service (Amazon S3) bucket. Your assignment is to produce a dashboard that meets your supervisorâ€™s requirements.\n",
    "\n",
    "To assist you, all of the previous labs have been provided in this workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lab steps\n",
    "\n",
    "To complete this lab, you will follow these steps:\n",
    "\n",
    "1. [Viewing the video files](#1.-Viewing-the-video-files)\n",
    "2. [Transcribing the videos](#2.-Transcribing-the-videos)\n",
    "3. [Normalizing the text](#3.-Normalizing-the-text)\n",
    "4. [Extracting key phrases and topics](#4.-Extracting-key-phrases-and-topics)\n",
    "5. [Creating the dashboard](#5.-Creating-the-dashboard)\n",
    "\n",
    "## Submitting your work\n",
    "\n",
    "1. In the lab console, choose **Submit** to record your progress and when prompted, choose **Yes**.\n",
    "\n",
    "1. If the results don't display after a couple of minutes, return to the top of these instructions and choose **Grades**.\n",
    "\n",
    "     **Tip**: You can submit your work multiple times. After you change your work, choose **Submit** again. Your last submission is what will be recorded for this lab.\n",
    "\n",
    "1. To find detailed feedback on your work, choose **Details** followed by **View Submission Report**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Useful information\n",
    "\n",
    "The following cell contains some information that might be useful as you complete this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bucket = 'c46255a638438l1748394t1w538120888142-labbucket-12figcw8iu648'\n",
    "job_data_access_role = 'arn:aws:iam::538120888142:role/service-role/c46255a638438l1748394t1w5-ComprehendDataAccessRole-1A1092NM0Q4C7'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Viewing the video files\n",
    "([Go to top](#Challenge-Lab-8:-Bringing-It-All-Together))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The source video files are located in the following shared Amazon Simple Storage Service (Amazon S3) bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Transcribing the videos\n",
    "([Go to top](#Challenge-Lab-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to implement your solution to transcribe the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/ s3://{bucket}/input/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from boto3 import client\n",
    "\n",
    "conn = client('s3') \n",
    "for key in conn.list_objects(Bucket=bucket)['Contents']:\n",
    "    print(key['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os, io, struct, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "from time import sleep\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transcribe_client = boto3.client(\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_files=[]\n",
    "transcribe_output_prefix = 'transcribed'\n",
    "for key in conn.list_objects_v2(Bucket=bucket, Prefix='input')['Contents']:\n",
    "    if 'temp' in key['Key']:\n",
    "        continue\n",
    "    object_name=key['Key']\n",
    "    media_input_uri = f's3://{bucket}/{object_name}'\n",
    "\n",
    "    #create the transcription job\n",
    "    job_uuid = uuid.uuid1()\n",
    "    transcribe_job_name = f\"transcribe-job-{job_uuid}\"\n",
    "    output_file = object_name.split('.')[0].replace(\" \",\"_\")\n",
    "    transcribe_output_filename = f'{transcribe_output_prefix}-{output_file}.txt'\n",
    "    output_files.append([transcribe_output_filename,object_name,\"\"])\n",
    "    print(f'{media_input_uri} transcribed to {transcribe_output_filename}')\n",
    "\n",
    "    response = transcribe_client.start_transcription_job(\n",
    "        TranscriptionJobName=transcribe_job_name,\n",
    "        Media={'MediaFileUri': media_input_uri},\n",
    "        MediaFormat='mp4',\n",
    "        LanguageCode='en-US',\n",
    "        OutputBucketName=bucket,\n",
    "        OutputKey=transcribe_output_filename\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(output_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "job=None\n",
    "while True:\n",
    "    job = transcribe_client.get_transcription_job(TranscriptionJobName = transcribe_job_name)\n",
    "    if job['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED','FAILED']:\n",
    "        break\n",
    "    print('.', end='')\n",
    "    sleep(20)\n",
    "        \n",
    "print(job['TranscriptionJob']['TranscriptionJobStatus'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "transcribed_text = []\n",
    "for transcribe_output_filename in output_files:\n",
    "    result = s3_client.get_object(Bucket=bucket, Key=transcribe_output_filename[0]) \n",
    "    data = json.load(result['Body']) \n",
    "    transcription = data['results']['transcripts'][0]['transcript']\n",
    "    transcribe_output_filename[2] = transcription\n",
    "\n",
    "print(output_files[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Normalizing the text\n",
    "([Go to top](#Challenge-Lab-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to perform any text normalization steps that are necessary for your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=output_files, columns=['OutputFile','Video','Transcription'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(content):\n",
    "    text = re.sub(r\"http\\S+\", \"\", content ) # Remove urls\n",
    "    text = text.lower() # Lowercase \n",
    "    text = text.strip() # Remove leading/trailing whitespace\n",
    "    text = re.sub('\\s+', ' ', text) # Remove extra space and tabs\n",
    "    text = re.sub('\\n',' ',text) # remove newlines\n",
    "    text = re.compile('<.*?>').sub('', text) # Remove HTML tags/markups:\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df['Transcription_normalized'] = df['Transcription'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 150)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Extracting key phrases and topics\n",
    "([Go to top](#Challenge-Lab-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to extract the key phrases and topics from the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "def upload_comprehend_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    \n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comprehend_file = 'comprehend_input.csv'\n",
    "prefix='capstone'\n",
    "upload_comprehend_s3_csv(comprehend_file, 'comprehend', df['Transcription_normalized'].str.slice(0,5000))\n",
    "test_url = f's3://{bucket}/{prefix}/comprehend/{comprehend_file}'\n",
    "print(f'Uploaded input to {test_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehend client information\n",
    "comprehend_client = boto3.client(service_name=\"comprehend\")\n",
    "\n",
    "# Other job parameters\n",
    "input_data_format = 'ONE_DOC_PER_LINE'\n",
    "job_uuid = uuid.uuid1()\n",
    "job_name = f\"kpe-job-{job_uuid}\"\n",
    "input_data_s3_path = test_url\n",
    "output_data_s3_path = f's3://{bucket}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Begin the inference job\n",
    "kpe_response = comprehend_client.start_key_phrases_detection_job(\n",
    "    InputDataConfig={'S3Uri': input_data_s3_path,\n",
    "                     'InputFormat': input_data_format},\n",
    "    OutputDataConfig={'S3Uri': output_data_s3_path},\n",
    "    DataAccessRoleArn=job_data_access_role,\n",
    "    JobName=job_name,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "# Get the job ID\n",
    "kpe_job_id = kpe_response['JobId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "job_name = f'entity-job-{job_uuid}'\n",
    "entity_response = comprehend_client.start_entities_detection_job(\n",
    "    InputDataConfig={'S3Uri': input_data_s3_path,\n",
    "                     'InputFormat': input_data_format},\n",
    "    OutputDataConfig={'S3Uri': output_data_s3_path},\n",
    "    DataAccessRoleArn=job_data_access_role,\n",
    "    JobName=job_name,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "# Get the job ID\n",
    "entity_job_id = entity_response['JobId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Creating the dashboard\n",
    "([Go to top](#Challenge-Lab-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to create the dashboard for your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use the link below to obtain the IP address of your computer. \n",
    "\n",
    "http://checkip.amazonaws.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copy the value displayed from the link above and replace the ip address below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_ip = \"72.21.198.0/24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install elasticsearch\n",
    "!pip install requests\n",
    "!pip install requests-aws4auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create an boto3 client for elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "es_client = boto3.client('es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following sets up an access policy so that only your ip address can access the elasticsearch dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "access_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"AWS\": \"*\"\n",
    "                },\n",
    "                \"Action\": \"es:*\",\n",
    "                \"Resource\": \"*\",\n",
    "                \"Condition\": { \"IpAddress\": { \"aws:SourceIp\": my_ip } }\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create the elasticsearch cluster using the following options:\n",
    "\n",
    "\n",
    "- **DomainName** - is the name of the elasticsearch cluster\n",
    "- **ElasticSearchClusterConfig** - specifies the instance type, the number of instances, whether a dedicated master is required, and if the cluster should be multi-zoned\n",
    "- **AccessPolicies** - contains the statement from above that restricts access to only your IP address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "response = es_client.create_elasticsearch_domain(\n",
    "    DomainName = 'nlp-lab',\n",
    "    ElasticsearchVersion = '7.9',\n",
    "    ElasticsearchClusterConfig={\n",
    "        \"InstanceType\": 'm3.large.elasticsearch',\n",
    "        \"InstanceCount\": 2,\n",
    "        \"DedicatedMasterEnabled\": False,\n",
    "        \"ZoneAwarenessEnabled\": False\n",
    "    },\n",
    "    AccessPolicies = json.dumps(access_policy)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Elasticsearch typically takes around 10 minutes to complete. Check the [Amazon ElasticSearch Service](https://console.aws.amazon.com/es/home?region=us-east-1#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get current job status\n",
    "kpe_job = comprehend_client.describe_key_phrases_detection_job(JobId=kpe_job_id)\n",
    "\n",
    "# Loop until job is completed\n",
    "waited = 0\n",
    "timeout_minutes = 30\n",
    "while kpe_job['KeyPhrasesDetectionJobProperties']['JobStatus'] != 'COMPLETED':\n",
    "    sleep(10)\n",
    "    waited += 10\n",
    "    assert waited//60 < timeout_minutes, \"Job timed out after %d seconds.\" % waited\n",
    "    print('.', end='')\n",
    "    kpe_job = comprehend_client.describe_key_phrases_detection_job(JobId=kpe_job_id)\n",
    "\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get current job status\n",
    "entity_job = comprehend_client.describe_entities_detection_job(JobId=entity_job_id)\n",
    "\n",
    "# Loop until job is completed\n",
    "waited = 0\n",
    "timeout_minutes = 30\n",
    "while entity_job['EntitiesDetectionJobProperties']['JobStatus'] != 'COMPLETED':\n",
    "    sleep(10)\n",
    "    waited += 10\n",
    "    assert waited//60 < timeout_minutes, \"Job timed out after %d seconds.\" % waited\n",
    "    print('.', end='')\n",
    "    entity_job = comprehend_client.describe_entities_detection_job(JobId=entity_job_id)\n",
    "\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once the results for both cells say 'Ready' you can proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Get the output for the Key Phrases detection Job by extracting the output location from the job and downloading it to the file system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kpe_comprehend_output_file = kpe_job['KeyPhrasesDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "print(f'output filename: {kpe_comprehend_output_file}')\n",
    "\n",
    "kpe_comprehend_bucket, kpe_comprehend_key = kpe_comprehend_output_file.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "\n",
    "s3r = boto3.resource('s3')\n",
    "s3r.meta.client.download_file(kpe_comprehend_bucket, kpe_comprehend_key, 'output-kpe.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, extract the file and rename the output so we know which file this is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the tar file\n",
    "import tarfile\n",
    "tf = tarfile.open('output-kpe.tar.gz')\n",
    "tf.extractall()\n",
    "# Rename the output\n",
    "!mv 'output' 'kpe_output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can repeat the above process for the entity detection job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "entity_comprehend_output_file = entity_job['EntitiesDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "print(f'output filename: {entity_comprehend_output_file}')\n",
    "\n",
    "entity_comprehend_bucket, entity_comprehend_key = entity_comprehend_output_file.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "\n",
    "s3r = boto3.resource('s3')\n",
    "s3r.meta.client.download_file(entity_comprehend_bucket, entity_comprehend_key, 'output-entity.tar.gz')\n",
    "\n",
    "# Extract the tar file\n",
    "import tarfile\n",
    "tf = tarfile.open('output-entity.tar.gz')\n",
    "tf.extractall()\n",
    "# Rename the output\n",
    "!mv 'output' 'entity_output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read in the data from the Key Phrases file into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "with open ('kpe_output', \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the data array into a dataframe. There are two columns, KeyPhrases and Line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kpdf = pd.DataFrame(data, columns=['KeyPhrases','Line'])\n",
    "kpdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can repeat the last 2 steps for the entities data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "with open ('entity_output', \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "entitydf = pd.DataFrame(data, columns=['Entities','Line'])\n",
    "entitydf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looking at the entities. the different detected entities are burried in the same fields. Depending on your scenario, you may want to split this out into separate columns for each entity type. To do this we can write a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_entities(entities, entity_type):\n",
    "    filtered_entities=[]\n",
    "    for entity in entities:\n",
    "        if entity['Type'] == entity_type:\n",
    "            filtered_entities.append(entity)\n",
    "    return filtered_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we can apply the function to each of the event types we want to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "        \n",
    "# df['plot_normalized'] = df['plot'].apply(normalize_text)    \n",
    "entitydf['location'] = entitydf['Entities'].apply(lambda x: extract_entities(x, 'LOCATION'))\n",
    "entitydf['organization'] = entitydf['Entities'].apply(lambda x: extract_entities(x, 'ORGANIZATION'))\n",
    "\n",
    "entitydf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With the results from Comprehend loaded into dataframes, it's time to merge everything together. The **Line** will merge together the results from Comprehend with the original dataframe.\n",
    "\n",
    "Start by setting the index on both results dataframes to the **Line** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "entitydf.set_index('Line', inplace = True)\n",
    "entitydf.sort_index(inplace=True)\n",
    "kpdf.set_index('Line', inplace=True)\n",
    "kpdf.sort_index(inplace=True)\n",
    "entitydf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, merge the **kpdf** dataframe with the **entitydf** dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m1 = kpdf.merge(entitydf, left_index=True, right_index=True)\n",
    "m1.sort_index(inplace=True)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "m1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now merge the **m1** dataframe with the original dataframe **df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mergedDf = df.merge(m1, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mergedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 50)\n",
    "mergedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "alive = es_client.describe_elasticsearch_domain(DomainName='nlp-lab')\n",
    "while alive['DomainStatus']['Processing']:\n",
    "    print('.', end='')\n",
    "    sleep(10)\n",
    "    alive = es_client.describe_elasticsearch_domain(DomainName='nlp-lab')\n",
    "    \n",
    "print('ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "es_domain = es_client.describe_elasticsearch_domain(DomainName='nlp-lab')\n",
    "es_endpoint = es_domain['DomainStatus']['Endpoint']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create an elasticsearch client using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "region= 'us-east-1' # us-east-1\n",
    "service = 'es' # IMPORTANT: this is key difference while signing the request for proxy endpoint.\n",
    "credentials = boto3.Session().get_credentials()\n",
    "\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "es = Elasticsearch(\n",
    "    hosts = [{'host': es_endpoint, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transcription = mergedDf.iloc[3,2]\n",
    "keyphrases = mergedDf.iloc[3,4]\n",
    "location = mergedDf.iloc[3,6]\n",
    "organization = mergedDf.iloc[3,7]\n",
    "movie_name = mergedDf.iloc[3,1]\n",
    "\n",
    "document = {\"name\": movie_name, \"transcription\": transcription, \"keyphrases\": keyphrases, \"location\":location, \"organization\": organization}\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from elasticsearch import helpers\n",
    "\n",
    "def gendata(start, stop):    \n",
    "    if stop>mergedDf.shape[0]:\n",
    "        stop = mergedDf.shape[0]\n",
    "    for i in range(start, stop):\n",
    "        yield {\n",
    "            \"_index\":'movies',\n",
    "            \"_type\": \"_doc\", \n",
    "            \"_id\":i, \n",
    "            \"_source\": {\"name\": mergedDf.iloc[i,1], \"transcription\": mergedDf.iloc[i,2], \"keyphrases\": mergedDf.iloc[i,4], \"location\":mergedDf.iloc[i,6], \"organization\": mergedDf.iloc[i,7]}\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, you need to get some up to date credentials for the elasticsearch service, then call **helpers.bulk** to upload the remaining documents. This should take around 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "es = Elasticsearch(\n",
    "    hosts = [{'host': es_endpoint, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "helpers.bulk(es, gendata(0,mergedDf.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create the Kibana Dashboard\n",
    "\n",
    "In this section, you will create a Kibana Dashboard to display and filter the results.\n",
    "\n",
    "First, grab the url for the Kibana dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'https://{es_endpoint}/_plugin/kibana')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Navigate to the kibana URL printed from the previous cell.\n",
    "1. Once the page loads, select **Dashboard**.\n",
    "1. Since this is the first time the dashboard is loaded, an **Index Pattern** will need to be defined. Select `Create index pattern`. \n",
    "1. Enter **movie*** as the **index pattern name**. You should see that the index pattern matches 1 source.\n",
    "1. Choose 'Next step'.\n",
    "1. Choose `Create index pattern`.\n",
    "1. You should see a table of fields displayed. If everything is working, you will see 28 fields.\n",
    "1. Choose the hamburger menu, and select 'Discover' from the list.\n",
    "1. In the available field list on the left, move to the **name** field and choose `add` when it appears.\n",
    "1. Choose `Save`.\n",
    "1. Enter **movies** as the title and choose `Save'.\n",
    "1. Choose the hamburger menu, and select 'Dashboard' from the list.\n",
    "1. Choose `Create new dashboard`.\n",
    "1. Choose `Add`.\n",
    "1. Select **Movies** from the list.\n",
    "1. Close the **Add Panels** pane.\n",
    "1. Choose `Create New`.\n",
    "1. Select **Tag Cloud** from the list of Visualizations.\n",
    "1. Choose **movie*** as the source.\n",
    "1. Under **Buckets** select `Add`, then choose **Tags**.\n",
    "1. Choose **Terms** as the `Aggregation`.\n",
    "1. Choose **keyphrases.Text.keyword** as the field.\n",
    "1. Enter **25** as the size.\n",
    "1. Select `Update`.\n",
    "1. Select `Save`.\n",
    "1. Enter **Key Phrases** as the `Title`.\n",
    "1. Choose `Save and return`\n",
    "\n",
    "1. Repeat steps 16-26 for the following fields:\n",
    "    - location.Text.keyword\n",
    "    - organization.Text.keyword\n",
    "\n",
    "1. Choose 'Create new'.\n",
    "1. Select **Metric** from the list of Visualizations.\n",
    "1. Choose **movie*** as the source.\n",
    "1. Select `Save`\n",
    "1. Enter **Total Documents** as the `Title`.\n",
    "1. Choose `Save and return`\n",
    "\n",
    "1. Select the calendar icon.\n",
    "1. From the **Commonly used** list, select **Today**.\n",
    "1. Select the calendar icon again and update the **Refresh every** to 5 seconds.\n",
    "1. Choose `Start`.\n",
    "\n",
    "1. Choose `Save`\n",
    "1. Enter **Movies** as the title.\n",
    "1. Choose `Save`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With the dashboard created, you can proceed to upload the remaining documents. There are some helper functions that allow you to do this quickly. First define a function that will create the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cleanup\n",
    "\n",
    "Once you have finished experimenting with elasticsearch, you can shutdown the cluster using the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "response = es_client.delete_elasticsearch_domain(\n",
    "    DomainName='nlp-lab'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Elasticsearch typically takes around 10 minutes to complete. While that is happening you can explore some other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have completed this lab, and you can now end the lab by following the lab guide instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Â©2021 Amazon Web Services, Inc. or its affiliates. All rights reserved. This work may not be reproduced or redistributed, in whole or in part, without prior written permission from Amazon Web Services, Inc. Commercial copying, lending, or selling is prohibited. All trademarks are the property of their owners.*\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "12bdb53ebf8de4a8c3e84b62f6391946884c7c7585d9344b706f290a85145ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}