{"cells": [{"source": ["# Lab 3.1: Extracting Text from Webpages and Images\n", "\n", "In this lab, you will use Beautiful Soup and Amazon Textract to extract text from the web and turn the results into a pandas dataframe.\n", "\n", "In the second part of the lab, you will experiment with Amazon Textract to extract text from images.\n", "\n", "\n", "## Lab steps\n", "\n", "To complete this lab, you will follow these steps:\n", "\n", "1. [Extracting information from a webpage](#1.-Extracting-information-from-a-webpage)\n", "2. [Extracting text from images](#2.-Extracting-text-from-images)\n", "    \n"], "cell_type": "markdown", "metadata": {}}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Upgrade dependencies\n", "!pip install --upgrade pip\n", "!pip install --upgrade sagemaker\n", "!pip install --upgrade beautifulsoup4\n", "!pip install --upgrade html5lib\n", "!pip install --upgrade requests\n", "!pip install --upgrade textract-trp"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Extracting information from a webpage\n", "([Go to top](#Lab-3.1:-Extracting-text-from-the-web))\n", "\n", "In this section, you will use Beautiful Soup to extract the titles, authors, summaries, published data, and hyperlinks from blog posts. The extracted text could then be used in a downstream NLP task, such as topic extraction, sentiment analysis, text-to-speech, or translation."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Start by importing both the **Beautiful Soup** and **requests** packages."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from bs4 import BeautifulSoup\n", "import requests"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The blog post you will parse is the [AWS Machine Learning blog](https://aws.amazon.com/blogs/machine-learning/) at https://aws.amazon.com/blogs/machine-learning/.\n", "\n", "Using your web browser, open the AWS Machine Learning page. \n", "\n", "Use the browser's *inspector mode* to discover the structure of the page. In Mozilla FireFox and Google Chrome, you can open the inspector by pressing CTRL+SHIFT+C. If you use a different browser, consult the browser documentation.\n", "\n", "View the different elements of the webpage by moving your pointer over the page. Move the pointer over the following elements, and see whether you can find the tags that are used to identify the informtion:\n", "\n", "* Title of the blog post\n", "* Author\n", "* Date published\n", "* Text summary\n", "* Hyperlink to the blog post\n", "\n", "Don't worry if you can't find all the tags. The following code walkthrough will help you find tags.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First, use the **requests** library to load the webpage. Before you proceed, confirm that the HTTP status code is *200*."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["page = requests.get('https://aws.amazon.com/blogs/machine-learning/')\n", "page.status_code"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load the **content** from the page into a **soup** object."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["soup = BeautifulSoup(page.content, 'html.parser')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["View the entire page by using the `soup.prettify()` function.\n", "\n", "**Note:** The content from the AWS Blogs page might be lengthy. To move to the next task, scroll down in this notebook."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "source": ["print(soup.prettify())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["All the elements on the page can be accessed using dot (.) notation. Thus, to view the title, you could use `soup.title`. If you want only the `text`, use the text element as follows:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(soup.title.text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When you used the inspector to search for tags on the AWS Blogs page, you might have found that blog-post content is organized/categorized/marked with `<article>` tags, which indicate a self-contained unit of content."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(soup.article.prettify())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Review the output. Can you find the title?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The title can be found at `soup.article.h2.span`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(soup.article.h2.span.prettify())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To display only the text, use the `text` property:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(soup.article.h2.span.text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Find the publish date of the article:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(soup.article.time.text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, extract the article summary:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(soup.article.section.p.text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The author name is in the footer. A blog post can have multiple authors. However, for now, retrieve only the *first author*:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(soup.article.footer.span.prettify())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The hyperlink to the full article text is the last piece of information that you must find:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(soup.article.section.a['href'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You have now identified all the relevant elements. You can find all the articles by using the `find_all()` function. You can then loop through the results and output information about the blog post, such as the title, author, and so on.\n", "\n", "For example, to find all the authors and then loop through them, the author, use `find_all()`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for article in soup.find_all('article'):\n", "    print('==========================================')\n", "    print(article.h2.span.text)\n", "    authors = article.footer.find_all('span', {\"property\":\"author\"})\n", "    print('by', end=' ')\n", "    for author in authors:\n", "        if author.span != None:\n", "            print(author.span.text, end=', ')\n", "    print(f'on {article.time.text}')\n", "    print(article.section.p.text)\n", "    print(article.section.a['href'])\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["After you figure out the data format, you can add the results to an array:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["blog_posts = []\n", "for article in soup.find_all('article'):\n", "    authors = article.footer.find_all('span', {\"property\":\"author\"})\n", "    author_text = []\n", "    for author in authors:\n", "        if author.span != None:\n", "            author_text.append(author.span.text)\n", "    blog_posts.append([article.h2.span.text, ', '.join(author_text), article.time.text, article.section.p.text, article.section.a['href'] ])\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, load the array into a pandas dataframe:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import time"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.DataFrame(blog_posts, columns=['title','authors','published','summary','link'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You must convert the **published** column to a `datetime` value."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df['published'] = pd.to_datetime(df['published'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Adjust the column width for pandas, and display the first five rows of the dataframe:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pd.options.display.max_rows\n", "pd.set_option('display.max_colwidth', None)\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that the data is in a pandas dataframe, you can use this data in downstream NLP tasks. You will come back to this data in Module 5."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Extracting text from images\n", "([Go to top](#Lab-3.1:-Extracting-text-from-the-web))\n", "\n", "In this section, you will extract the text from an image by using Amazon Textract.\n", "\n", "For this exercise, you will use the following simple image. This file was loaded into Amazon Simple Storage Service (Amazon S3) when you started the lab.\n", "\n", "![Image of a simple document](../s3/simple-document-image.jpg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Start by importing the library for the AWS SDK for Python (Boto3)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import boto3"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Setup the variables for the bucket and document name."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Document\n", "s3BucketName = \"c51302a798363l1767466t1w753256443787-labbucket-vzr6xg8irt81\"\n", "documentName = \"lab31/simple-document-image.jpg\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Extract text from the image by using Amazon Textract to call an application programming interface (API)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Amazon Textract client\n", "textract = boto3.client('textract')\n", "\n", "# Call Amazon Textract\n", "response = textract.detect_document_text(\n", "    Document={\n", "        'S3Object': {\n", "            'Bucket': s3BucketName,\n", "            'Name': documentName\n", "        }\n", "    })\n", "\n", "print(response)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The response looks unformatted, but the **Blocks** list contains the key information that you need. \n", "\n", "Extract this information from the **Blocks** list:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Print text\n", "print(\"\\nText\\n========\")\n", "text = \"\"\n", "for item in response[\"Blocks\"]:\n", "    if item[\"BlockType\"] == \"LINE\":\n", "        print ('\\033[94m' +  item[\"Text\"] + '\\033[0m')\n", "        text = text + \" \" + item[\"Text\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You have now extracted the text from the image. You can use this text in a downstream NLP task."]}, {"cell_type": "markdown", "metadata": {}, "source": ["You will now experiment with one additional image. This image contains *tables* of text.\n", "\n", "![Image of Employment Application](../s3/employmentapp.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set the new document name:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Document\n", "documentName = \"lab31/employmentapp.png\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Call the Amazon Textract API again. However, this time, specify the **TABLES** feature type:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Amazon Textract client\n", "\n", "response = textract.analyze_document(\n", "    Document={\n", "        'S3Object': {\n", "            'Bucket': s3BucketName,\n", "            'Name': documentName\n", "        }\n", "    },\n", "    FeatureTypes=[\"TABLES\"])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Parse the table by using the Amazon Textract results parser (**textract-trp**).\n", "\n", "**Note:** You installed the Amazon Textract results parser when you ran the `pip install --upgrade textract-trp` command at the start of this notebook."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from trp import Document\n", "doc = Document(response)\n", "\n", "for page in doc.pages:\n", "    for table in page.tables:\n", "        for r, row in enumerate(table.rows):\n", "            for c, cell in enumerate(row.cells):\n", "                print(\"Table[{}][{}] = {}\".format(r, c, cell.text))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You have now extracted the text from a different image, and you could continue to process it further, if needed."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Congratulations!\n", "\n", "You have completed this lab, and you can now end the lab by following the lab guide instructions."]}, {"cell_type": "markdown", "metadata": {}, "source": ["*\u00a92021 Amazon Web Services, Inc. or its affiliates. All rights reserved. This work may not be reproduced or redistributed, in whole or in part, without prior written permission from Amazon Web Services, Inc. Commercial copying, lending, or selling is prohibited. All trademarks are the property of their owners.*"]}], "metadata": {"instance_type": "ml.t3.medium", "kernelspec": {"display_name": "conda_python3", "language": "python", "name": "conda_python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 4}