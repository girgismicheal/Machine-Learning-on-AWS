{"cells": [{"source": ["# Lab 3.2: Processing Text\n", "\n", "In this lab, you will look at simple techniques to clean and prepare text data for modeling with machine learning (ML).\n", "\n", "\n", "## Lab steps\n", "\n", "To complete this lab, you will follow these steps:\n", "\n", "1. [Working with simple text-cleaning processes](#1.-Working-with-simple-text-cleaning-processes)\n", "2. [Working with lexicon-based text processing](#2.-Working-with-lexicon-based-text-processing)\n", "        \n"], "cell_type": "markdown", "metadata": {}}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Upgrade dependencies\n", "!pip install --upgrade pip\n", "!pip install --upgrade scikit-learn\n", "!pip install --upgrade sagemaker"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. <a name=\"1\">Working with simple text-cleaning processes</a>\n", "([Go to top](#Lab-3.2:-Processing-Text))\n", "\n", "In this section, you will do some general-purpose text cleaning. The following methods for cleaning can be extended, depending on the application."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["text = \"   This is a message to be cleaned. It might involve some things like: <br>, ?, :, ''  adjacent spaces, and tabs     .  \"\n", "print(text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First, change the text so that it's all lowercase:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["text = text.lower()\n", "print(text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, remove leading whitespace or trailing whitespace with the following code:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["text = text.strip()\n", "print(text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use a regular expression to remove HTML tags or markup:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import re\n", "\n", "text = re.compile('<.*?>').sub('', text)\n", "print(text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Replace punctuation with a space. Be careful with this task. Depending on the application, punctuation can actually be useful. For example, punctuation might affect the positive or negative meaning of a sentence."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import re, string\n", "\n", "text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n", "print(text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove any extra spaces and tabs:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import re\n", "\n", "text = re.sub('\\s+', ' ', text)\n", "print(text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Working with lexicon-based text processing\n", "([Go to top](#Lab-3.2:-Processing-Text))\n", "\n", "In the previous section, you used some general-purpose text pre-processing methods. Lexicon-based methods are usually applied *after* the common text-processing methods. They are used to normalize sentences in the dataset. *Normalization* means putting words into a similar format that will also enhance the similarities (if any) between sentences."]}, {"cell_type": "markdown", "metadata": {}, "source": ["For this example, you must install some packages:\n", "\n", "- punkt - A pretrained sentence tokenizer for the English language\n", "- averaged_perceptron_tagger - A part-of-sentence (POS) tagger\n", "- wordnet - A large database of English words that can be used to find the meanings of words, synonyms, antonyms, and more\n", "\n", "Run the following cell:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import nltk\n", "\n", "nltk.download('punkt')\n", "nltk.download('averaged_perceptron_tagger')\n", "nltk.download('wordnet')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Stopword removal\n", "Some words in sentences can occur very frequently, and they don't contribute too much to the overall meaning of the sentences. Typically, you would use list of these words and remove them from each sentence. For example, stopwords include: *a*, *an*, *the*, *this*, *that*, *is*, *it*, *to*, and *and*."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Use a tokenizer from the NLTK library\n", "import nltk\n", "from nltk.tokenize import word_tokenize\n", "\n", "filtered_sentence = []\n", "\n", "# Stopword lists can be adjusted for your problem\n", "stopwords = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n", "\n", "# Tokenize the sentence\n", "words = word_tokenize(text)\n", "for w in words:\n", "    if w not in stopwords:\n", "        filtered_sentence.append(w)\n", "text = \" \".join(filtered_sentence)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Stemming words\n", "Stemming is a rule-based system for converting words into their root form. It removes suffixes from words. This process helps enhance similarities (if any) between sentences. \n", "\n", "Examples:\n", "\n", "\"jumping\", \"jumped\" -> \"jump\"\n", "\n", "\"cars\" -> \"car\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Use a tokenizer and stemmer from the NLTK library\n", "import nltk\n", "from nltk.tokenize import word_tokenize\n", "from nltk.stem import SnowballStemmer\n", "\n", "# Initialize the stemmer\n", "snow = SnowballStemmer('english')\n", "\n", "stemmed_sentence = []\n", "# Tokenize the sentence\n", "words = word_tokenize(text)\n", "for w in words:\n", "    # Stem the word/token\n", "    stemmed_sentence.append(snow.stem(w))\n", "stemmed_text = \" \".join(stemmed_sentence)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(stemmed_text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["From the previous cell, you can see that the stemming operation is *not* perfect. It generated some mistakes, such as *messag*, *involv*, and *adjac*. Stemming is a rule-based method that sometimes mistakenly remove suffixes from words. Nevertheless, it runs quickly."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Lemmatizing words\n", "If you are not satisfied with the result of stemming, you can use lemmatization instead. It usually requires more work, but it gives better results."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Importing the necessary functions\n", "import nltk\n", "from nltk.tokenize import word_tokenize\n", "from nltk.corpus import wordnet\n", "from nltk.stem import WordNetLemmatizer\n", "\n", "# Initialize the lemmatizer\n", "wl = WordNetLemmatizer()\n", "\n", "# This is a helper function to map NTLK position tags\n", "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n", "def get_wordnet_pos(tag):\n", "    if tag.startswith('J'):\n", "        return wordnet.ADJ\n", "    elif tag.startswith('V'):\n", "        return wordnet.VERB\n", "    elif tag.startswith('N'):\n", "        return wordnet.NOUN\n", "    elif tag.startswith('R'):\n", "        return wordnet.ADV\n", "    else:\n", "        return wordnet.NOUN\n", "\n", "lemmatized_sentence = []\n", "# Tokenize the sentence\n", "words = word_tokenize(text)\n", "# Get position tags\n", "word_pos_tags = nltk.pos_tag(words)\n", "# Map the position tag and lemmatize the word or token\n", "for idx, tag in enumerate(word_pos_tags):\n", "    lemmatized_sentence.append(wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))\n", "\n", "lemmatized_text = \" \".join(lemmatized_sentence)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(lemmatized_text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can use the tasks you completed in this notebook for many of the business problems that you will work on in this course. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Congratulations!\n", "\n", "You have completed this lab, and you can now end the lab by following the lab guide instructions."]}, {"cell_type": "markdown", "metadata": {}, "source": ["*\u00a92021 Amazon Web Services, Inc. or its affiliates. All rights reserved. This work may not be reproduced or redistributed, in whole or in part, without prior written permission from Amazon Web Services, Inc. Commercial copying, lending, or selling is prohibited. All trademarks are the property of their owners.*"]}], "metadata": {"instance_type": "ml.t3.medium", "kernelspec": {"display_name": "conda_python3", "language": "python", "name": "conda_python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 4}